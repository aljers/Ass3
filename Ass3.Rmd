---
title: "Ass2"
author: "Jiaxuan Lu"
date: "2022-10-27"
output: pdf_document
---
```{r}
library(tidyverse)
library(patchwork)
library(lmtest)
```
## Q1
```{r}
hw2_data_q1 <- read.csv('https://raw.githubusercontent.com/hgweon2/ss3859/master/hw2-data-2.csv')
lm_q1 <- lm(y~x, data = hw2_data_q1)
# par(mfrow = c(2,2))
# plot(lm_q1)
plot(fitted(lm_q1), resid(lm_q1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)
```
To check the linearity, normality and equal variance assumptions. We made the residual plot. The orange horizontal line is the zero residual line. From residual plot, the mean of residual varies systematically, therefore the linearity assumption is violated. At the same time, the spread of residual is roughly same (constant), which means the variance of residuals is constant, hence the equal variance assumption holds. 
```{r}
qqnorm(hw2_data_q1$y)  
qqline(hw2_data_q1$y, col = "dodgerblue", lwd = 2)
#hist(hw2_data$y,breaks=100,col="darkorange")
```
To check the normality, we use QQ plot. In this case, the sample quantile has shorter tail in both side. The normality is violated since the sample quantile does not give a linear pattern. 


```{r}
bptest(lm_q1)
```
Making conclusion from residual plot to diagnostic equal variance assumption is an informal way. We then introduced Breusch-Pagan test for checking the equal variance assumption. It assumed variance for each residual is all constant in null hypothesis. The test statistic is $0.0090726$ with corresponding p-value $0.9241$. Given $\alpha=0.05$, we do not have enough evidence against the null hypothesis. The equal variance assumption holds. 

```{r}
shapiro.test(resid(lm_q1))
```
Aside from QQ plot, we also used a formal test for the normality: Shapiro-Wilk Normality Test. In null hypothesis, we assume all the residuals are from a normal distribution. The test statistic is $W=\frac{\sum {\left(a_i e_i\right)}^2}{\sum e_i^2}$ where $e_i$ is the $i$th smallest residual and $a_i$ is the coefficient for $e_i$. The test statistic gives $W=0.97905$ with corresponding $p\text{-}value=0.1121$. Given $\alpha=0.05<0.1121$, we do not have enough evidence against the null hypothesis. The normality assumption holds.
## Q2

```{r}
hw2_data_q2 <- read.csv('https://raw.githubusercontent.com/hgweon2/ss3859/master/hw2-data-3.csv')
lm_q2 <- lm(y~x, data = hw2_data_q2)
# par(mfrow = c(2,2))
# plot(lm_q1)
plot(fitted(lm_q2), resid(lm_q2), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)
```
We repeated the steps in question 1. From this residual, we can see that residuals are allocated around zero, so the linearity assumption holds. Beside, residuals get more spread as the fitted value $\hat Y$ increases, it means the variance of residuals is not constant, and the equal variance assumption is violated. 
```{r}
qqnorm(hw2_data_q2$y)  
qqline(hw2_data_q2$y, col = "dodgerblue", lwd = 2)
#hist(hw2_data$y,breaks=100,col="darkorange")
```
Next, we checked the normal assumption by QQ plot. We can see a heavier right tail and shorter left tail, the distribution is skew to the right. It shows evidence against the normality assumption. According to the last part of question 1, we have to run a formal test to determine the significance of this evidence and make conclusion.
```{r}
bptest(lm_q2)
```
The BP test in this case returns a test statistic 22.542 with corresponding $p\text{-}value=2.056\times10^{-6}$. Given $\alpha = 0.05$, we have enough evidence against the null hypothesis. The equal variance assumption is violated.

```{r}
shapiro.test(resid(lm_q2))
```
The SW test in this case returns a test statistic $W=0.95913$ with corresponding $p\text{-}value=0.003487$. Given $\alpha = 0.05$, we have enough evidence against the null hypothesis. The normality assumption is violated.
## Q3
## (a)
```{r}
hw2_data_q3 <- read.csv('https://raw.githubusercontent.com/hgweon2/data/main/hw3-data.txt')
pairs(~., data = hw2_data_q3)
```
## (b)
```{r}
lm_q3 <- lm(y~x1+x2, data = hw2_data_q3)
# check by graphs 
# residual plot
plot(fitted(lm_q3), resid(lm_q3), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)
```
From this residual plot, we observe that residuals have a quadratic pattern with the extend of spread be th same for any fitted variance. In other word, given any fitted, the residual mean varies but the residual variances are the same. Hence, we conconclude that the assumption of equal variance holds while the linearity assumption is violated. 
```{r}
# QQ plot
qqnorm(hw2_data_q3$y)  
qqline(hw2_data_q3$y, col = "dodgerblue", lwd = 2)
```
This QQ plot shows that residual has shorter tail in both side than a (standard) normal distribution. This finding gives us evidence against the normality assumption. (normality assumption may not holds)
```{r}
bptest(lm_q3)
```
We carried BP test for checking the equal variance assumption. We have test statistic $0.094601$ with corresponding $p\text{-}value=0.9538$. Given $\alpha=0.05$, we do not have enough evidence against the null hypothesis. Therefore the equal variance assumption is violated.
```{r}
shapiro.test(resid(lm_q3))
```
We use SW test for checking the normality assumption. We have test statistic $W=0.95915$ with corresponding $p\text{-}value=1.603\times 10^{-5}$. Given $\alpha=0.05$, $0.05 >1.603\times 10^{-5}$, we now have enough evidence against the null hypothesis. Therefore the normality assumption holds.
## (c)
```{r}
# Obtains Cook's distances
lm_q3_cd = cooks.distance(lm_q3)
# We have 4 influential observations
sum(lm_q3_cd > 4/length(lm_q3_cd))
# ASSUME that those points are simple measurement errors
# Eliminate the points from mtcars and store the rest into mtcars2
inf_i = which(lm_q3_cd > 4/length(lm_q3_cd))
print(inf_i)
```
An influential point is determined according to the Cook's Distance: $D_i=\frac{\sum^{n}_{j=1}\left(\hat Y_j-\hat Y_j^{-i}\right)^2}{p\cdot \hat\sigma^2}$, where $\hat Y_j^{-i}$ is the fitted value for the $j$th observation when $i$th observation was removed from the data. In this question, we set the threshold to be $\frac{4}{n}$, where n is the number of observations. By this rule, we say $i$th observation is influential if we have $D_i>\frac{4}{n}$. In summary, 14 points are detected as influential points. Their index are also printed above. 

## (d)
```{r}
out_i = which(abs(rstandard(lm_q3)) > 2)
intersect(inf_i, out_i)
length(intersect(inf_i, out_i))
```
An outlier is a point that does not fit the model well. We detect it by standardized residuals:$r_i=\frac{e_i}{\hat\sigma\sqrt{1-h_{ii}}}$. Usually, we set 2 to be the threshold, the $i$th observation is an outlier if $\left|r_i\right|>2$. To search for the outliers from all the influential points is equivalent to find their intersection. In summary, 5 points from influential points are detected as outlier

## (e)
```{r}
hw2_data_q3_new = hw2_data_q3[-inf_i,]
# Fit mtcars2
lm_q3_new = lm(y~x1+x2, data = hw2_data_q3_new)

# Residual plot and normal qq plot
par(mfrow=c(1,2))
plot(fitted(lm_q3_new), resid(lm_q3_new), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residual",cex=2,
     main = "Error removed: Fitted vs Res.")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(lm_q3_new), col = "grey",pch=20,cex=2)
qqline(resid(lm_q3_new), col = "dodgerblue", lwd = 2)
```
Now, all the influential points are removed and we fitted a new model based on our new set of data. From residual plot, the quadratic 

```{r}
# bp test
bptest(lm_q3_new)
```


```{r}
# Shapiro test
shapiro.test(resid(lm_q3_new))
```

